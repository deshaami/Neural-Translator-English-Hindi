{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"English_Hindi_Clean_New.csv\", encoding='utf-8')\n",
    "\n",
    "# Preprocess: vocabulary creation\n",
    "all_eng_words = set(word for sentence in data['English'] for word in sentence.split())\n",
    "all_hin_words = set(word for sentence in data['Hindi'] for word in sentence.split())\n",
    "\n",
    "# Filter long sentences\n",
    "data['len_eng_sen'] = data['English'].apply(lambda x: len(x.split()))\n",
    "data['len_hin_sen'] = data['Hindi'].apply(lambda x: len(x.split()))\n",
    "data = data[(data['len_eng_sen'] <= 20) & (data['len_hin_sen'] <= 20)]\n",
    "\n",
    "# Max lengths\n",
    "max_len_src = max(data['len_eng_sen'])\n",
    "max_len_tar = max(data['len_hin_sen'])\n",
    "\n",
    "# Vocabulary\n",
    "inp_words = sorted(list(all_eng_words))\n",
    "tar_words = sorted(list(all_hin_words))\n",
    "num_enc_toks = len(inp_words) + 1\n",
    "num_dec_toks = len(tar_words) + 1\n",
    "\n",
    "# Token indices\n",
    "inp_tok_idx = {word: i+1 for i, word in enumerate(inp_words)}\n",
    "tar_tok_idx = {word: i+1 for i, word in enumerate(tar_words)}\n",
    "rev_inp_tok_idx = {i: word for word, i in inp_tok_idx.items()}\n",
    "rev_tar_tok_idx = {i: word for word, i in tar_tok_idx.items()}\n",
    "\n",
    "# Split data\n",
    "X, y = data['English'], data['Hindi']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generator function\n",
    "def generate_batch(X, y, batch_size):\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            enc_input = np.zeros((batch_size, max_len_src), dtype='float32')\n",
    "            dec_input = np.zeros((batch_size, max_len_tar), dtype='float32')\n",
    "            dec_target = np.zeros((batch_size, max_len_tar, num_dec_toks), dtype='float32')\n",
    "\n",
    "            for i, (inp, tar) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(inp.split()):\n",
    "                    enc_input[i, t] = inp_tok_idx.get(word, 0)\n",
    "                for t, word in enumerate(tar.split()):\n",
    "                    if t < max_len_tar:\n",
    "                        dec_input[i, t] = tar_tok_idx.get(word, 0)\n",
    "                    if t > 0 and t < max_len_tar:\n",
    "                        dec_target[i, t-1, tar_tok_idx.get(word, 0)] = 1.0\n",
    "\n",
    "            yield [enc_input, dec_input], dec_target\n",
    "\n",
    "# Model parameters\n",
    "latent_dim = 250\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(num_enc_toks, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "_, state_h, state_c = LSTM(latent_dim, return_state=True)(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_dec_toks, latent_dim, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_dec_toks, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Train\n",
    "train_steps = len(X_train) // batch_size\n",
    "val_steps = len(X_test) // batch_size\n",
    "\n",
    "model.fit(\n",
    "    generate_batch(X_train, y_train, batch_size),\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=generate_batch(X_test, y_test, batch_size),\n",
    "    validation_steps=val_steps\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
